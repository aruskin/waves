dpois(10, 15)
dpois(1, 15)
dpois(2, 15)
dpois(10, 15)
dpois(15, 15)
exp(-15)
exp(-15)*(15^10)/factorial(15)
exp(-15)*(15^10)/factorial(10)
sum(dpois(1:10, 15))
?qt
qt(.95, 8)
qt(.95, 8)*30/3
qt(.95, 8)*30/3 + 1100
-qt(.95, 8)*30/3 + 1100
-qt(.95, 9)*30/3 + 1100
-qt(.975, 9)*30/3 + 1100
-qt(.975, 8)*30/3 + 1100
qt(.975, 8)*30/3 + 1100
6/qt(.975, 8)
qt(.975, 18) * sqrt((9*(.60+.68))/18)*sqrt(.2)
(3-5) + qt(.975, 18) * sqrt((9*(.60+.68))/18)*sqrt(.2)
(3-5) - qt(.975, 18) * sqrt((9*(.60+.68))/18)*sqrt(.2)
(4-6) - qt(.975, 198) * sqrt((99*(0.5^2 + 2^2 ))/198)*sqrt(.02)
(4-6) + qt(.975, 198) * sqrt((99*(0.5^2 + 2^2 ))/198)*sqrt(.02)
(6-4) + qt(.975, 198) * sqrt((99*(0.5^2 + 2^2 ))/198)*sqrt(.02)
(6-4) - qt(.975, 198) * sqrt((99*(0.5^2 + 2^2 ))/198)*sqrt(.02)
(-3 - 1) - qt(.95, 16) * sqrt((8*(1.5^2 + 1.8^2 ))/16)*sqrt(2/9)
(-3 - 1) + qt(.95, 16) * sqrt((8*(1.5^2 + 1.8^2 ))/16)*sqrt(2/9)
z <- data.frame(base = c(140, 138, 150, 148, 135), week2 = c(132, 135, 151, 146, 130))
z
?t.test
t.test(x = z$base, y = z$week2, paired=TRUE)
?power.t.test
power.t.test(n=9, delta = 1100, sd = 30, sig.level = 0.05)
power.t.test(n=9, sd = 30, sig.level = 0.05, type="one.sample", alternative = "one.sided")
power.t.test(n=9, sd = 30, sig.level = 0.05, power = 0.5, type="one.sample", alternative = "one.sided")
1100 -18
1100 + 18
power.t.test(n=9, sd = 30, sig.level = 0.05, power = 0.5, type="one.sample", alternative = "two.sided")
power.t.test(n=9, sd = 30, sig.level = 0.05, power = 0.51, type="one.sample", alternative = "two.sided")
power.t.test(n=9, sd = 30, sig.level = 0.05, power = 0.49, type="one.sample", alternative = "two.sided")
t.test(x = c(1, 1, 1, 0), alternative = "g")
sqrt(var(c(1, 1, 1, 0)))
binom.test(x = 3, n=4)
binom.test(x = 3, n=4, alternative = "greater")
binom.test(x = 10, n=1787, p = 1/100, alternative = "less")
?power.t.test
?t.power
mean.diff = -5 - 1
df = 9 + 9 - 2
pooled.var = (1.5^2 * 8 + 1.8^2 * 8) / df
se.diff = sqrt(pooled.var/9 + pooled.var/9)
t.obt = mean.diff/se.dif
t.obt = mean.diff/se.diff
t.obt
p.value = 2 * pt(abs(t.obt), df=df, lower.tail = FALSE)
p.value
power.t.test(n=100, delta = .01, sd=.04)
power.t.test(n=100, delta = .01, sd=.04, alternative = "one.sided")
power.t.test(n=100, delta = .01, sd=.04, type= "one.sample", alternative = "one.sided")
power.t.test(delta = .01, sd=.04, power=.9, type= "one.sample", alternative = "one.sided")
library(ggplot2)
library(dplyr)
library(ggrepel)
runtime_directors <- read.csv("runtime_directors.csv")
get_letter_freqs <- function(string){
require(dplyr)
data <- strsplit(string, split="") %>%
table() %>%
as.data.frame()
colnames(data) <- c("letter", "freq")
data
}
min_necessary_letters <- function(wordlist){
require(dplyr)
lapply(wordlist, get_letter_freqs) %>%
bind_rows(.) %>%
group_by(letter) %>%
summarise(occ = max(freq)) %>%
ungroup() %>%
summarise(total = sum(occ))
}
which_letters <- function(wordlist){
require(dplyr)
lapply(wordlist, get_letter_freqs) %>%
bind_rows(.) %>%
group_by(letter) %>%
summarise(occ = max(freq))
}
seven.dirty.words <- c("shit", "piss", "fuck", "cunt", "cocksucker",
"motherfucker", "tits")
min_necessary_letters(seven.dirty.words) # 21--and Boggle only gives us 16
six.dirty.words <- combn(dirty.words, 6)
six.dirty.words <- combn(seven.dirty.words, 6)
apply(six.dirty.words, 2, min_necessary_letters) %>% unlist(.) # all still > 16
five.dirty.words <- combn(seven.dirty.words, 5)
z <- apply(five.dirty.words, 2, min_necessary_letters)
potential.fives <- five.dirty.words[,which(unlist(z) <= 16)]
apply(potential.fives, 2, which_letters)
means <- sapply(1:1000, rexp(40, lambda=0.2))
means <- sapply(1:1000, rexp(n = 40, lambda=0.2))
mean(rexp(n = 40, lambda=0.2))
?rexp
mean(rexp(40, 0.2))
means <- sapply(1:1000, mean(rexp(40, 0.2)))
means <- sapply(1:1000, function(i) mean(rexp(40, 0.2)))
hist(means)
shiny::runApp('Coding Adventures/Cross-stitch/xStitch-Shiny')
library(XML)
doc.html = htmlTreeParse('http://gutenberg.net.au/ebooks02/0201091h.html',
useInternal = TRUE)
doc.text = unlist(xpathApply(doc.html, '//p', xmlValue))
head(doc.text)
doc.text[20:30]
doc.text = gsub('\\n', ' ', doc.text)
doc.text[20:30]
doc.text <- gsub('\r', '', doc.text)
doc.text[20:30]
doc.text[100:110]
doc.text[1:20]
waves.text <- doc.text[-grep("Â ", doc.text)]
waves.text[1:10]
grep("Â ", doc.text)
grep("Â ", doc.text)head(doc.text)
head(doc.text)
which(doc.text == doc.text[1])
bad.lines <- which(doc.text == doc.text[1])
waves.text <- doc.text[-bad.line]
waves.text <- doc.text[-bad.lines]
heaD(waves.text)
head(waves.text)
head(waves.text)
waves.text <- waves.text[2:length(waves.text)]
tail(waves.text)
waves.text <- waves.text[1:length(waves.text)-2]
waves.text <- waves.text[1:(length(waves.text)-2)]
tail(waves.text)
name_locs <- str_locate(pattern = "\' said [A-Z][a-z]*. \'", waves.text)
library(stringr)
name_locs <- str_locate(pattern = "\' said [A-Z][a-z]*. \'", waves.text)
head(name_locs)
id <- sapply(1:length(waves.text),
function(i) substr(waves.text,
names_loc[i, 1],
names_loc[i, 2])) %>%
gsub("\' said ", "", .) %>%
gsub(". \'", "", .)
id <- sapply(1:length(waves.text),
function(i) substr(waves.text,
name_locs[i, 1],
name_locs[i, 2])) %>%
gsub("\' said ", "", .) %>%
gsub(". \'", "", .)
head(id)
id <- sapply(1:length(waves.text),
function(i) substr(waves.text,
name_locs[i, 1],
name_locs[i, 2]))
id <- sapply(1:length(waves.text),
function(i) substr(waves.text[i],
name_locs[i, 1],
name_locs[i, 2])) %>%
gsub("\' said ", "", .) %>%
gsub(". \'", "", .)
head(id)
waves <- grep("^[^\']", waves.text)
waves.passages <- grep("^[^\']", waves.text)
id[waves.passages] <- "WAVES"
curr_speaker <- "WAVES"
for(i in 1:length(id)){
if(!is.na(id[i])) curr.speaker <- id[i]
else id[i] <- curr.speaker
}
id[1:100]
waves.text <- gsub("\' said [A-Z][a-z]*. \'", " ", waves.text)
head(waves.text)
library(tm)
install.packages("tm")
library(tm)
install.packages("tokenizer")
library(tokenizer)
install.packages("tokenizers")
library(tokenizers)
tokenize_words(waves.text[1])
tokenize_word_stems(waves.text[1])
tokenize_words(waves.text[1])
tokenize_words(waves.text[1], simplify = TRUE)
tokenize_words(waves.text[20])
waves.text[20]
waves.text[30]
waves.text[50]
tokenize_words(waves.text[50])
waves.words <- lapply(tokenize_words(waves.text)) %>% unlist %>% unique
waves.words <- lapply(waves.text, tokenize_words) %>% unlist %>% unique
head(waves.words)
word.matrix <- matrix(nrow=length(waves.text), ncol=length(waves.words))
colnames(word.matrix) <- waves.words
waves.words <- lapply(waves.text, tokenize_words)
head(waves.words)
waves.words <- lapply(waves.text, tokenize_words) %>%
lapply(., table)
head(waves.words)
all.words <- lapply(waves.text, tokenize_words) %>% unlist %>% unique %>% sort
waves.text.words <- lapply(waves.text, tokenize_words) %>%
lapply(., table)
rbind.fill(waves.text.words[1], waves.text.words[2])
rbind.fill(as.data.frame(waves.text.words[1]), as.data.frame(waves.text.words[2]))
rbind.fill(t(as.data.frame(waves.text.words[1])), t(as.data.frame(waves.text.words[2])))
as.data.frame(waves.text.words[1])
as.data.frame(t(waves.text.words[1]))
t(as.data.frame(waves.text.words[1]))
mode(t(as.data.frame(waves.text.words[1])))
as.data.frame.matrix(t(waves.text.words[1]))
as.data.frame(waves.text.words[1])
as.data.frame.matrix(waves.text.words[1])
as_data_frame(waves.text.words[1])
library(tidyverse)
install.packages(tidyverse)
install.packages("tidyverse")
library(tidyverse)
as_data_frame(waves.text.words[1])
waves.text.words[1]
waves.text.words[[1]]
as_data_frame(waves.text.words[[1]])
t(as_data_frame(waves.text.words[[1]]))
as_data_frame(t(waves.text.words[[1]])))
as_data_frame(t(waves.text.words[[1]]))
as_data_frame(waves.text.words[[1]])
as.data.frame(waves.text.words[[1]])
x <-as.data.frame(waves.text.words[[1]])
rownames(x) <- x$Var1
x$Var1 <- NULL
t(x)
waves.text.words <- lapply(waves.text, tokenize_words) %>%
lapply(., table) %>%
lapply(., function(x) {
x <- as.data.frame(x)
rownames(x) <- x$Var1
x$Var1 <- NULL
t(x)
})
head(waves.text.words)
waves.text.words <- lapply(waves.text, tokenize_words) %>%
lapply(., table) %>%
lapply(., function(x) {
x <- as.data.frame(x)
rownames(x) <- x$Var1
x$Var1 <- NULL
t(x)
}) %>%
rbind.fill
mode(t(x))
t9x
t(x)
is.data.frame(t(x))
waves.text.words <- lapply(waves.text, tokenize_words) %>%
lapply(., table) %>%
lapply(., function(x) {
x <- as.data.frame(x)
rownames(x) <- x$Var1
x$Var1 <- NULL
as.data.frame(t(x))
}) %>%
rbind.fill
View(waves.text.words)
waves.text.words$speaker <- id
waves.text.words$id <- paste0(id, 1:length(id))
rhoda <- filter(waves.text.words, speaker="Rhoda")
rhoda <- filter(waves.text.words, speaker=="Rhoda")
View(rhoda)
?regexp
library(XML)
doc.html <- htmlTreeParse('http://gutenberg.net.au/ebooks02/0201091h.html',
useInternal = TRUE)
library(dplyr)
doc.text <-  unlist(xpathApply(doc.html, '//p', xmlValue)) %>%
gsub('\\n', ' ', doc.text) %>%
gsub('\r', '', doc.text)
doc.text <-  unlist(xpathApply(doc.html, '//p', xmlValue)) %>%
gsub('\\n', ' ', .) %>%
gsub('\r', '', .)
bad.lines <- which(doc.text == doc.text[1])
waves.text <- doc.text[-bad.lines]
head(waves.text)
waves.text <- waves.text[2:length(waves.text)]
tail(waves.text)
waves.text <- waves.text[1:(length(waves.text)-2)]
library(stringr)
name_locs <- str_locate(pattern = "\' said [A-Z][a-z]*[:punct:]", waves.text)
id <- sapply(1:length(waves.text),
function(i) substr(waves.text[i],
name_locs[i, 1],
name_locs[i, 2])) %>%
gsub("\' said ", "", .) %>%
gsub("[:punct:]*", "", .)
id
gsub("[:punct:]", "", "Susan...")
gsub("[:punct:]", "", "Susan...", perl=TRUE)
gsub("[:punct:]", "", "Susan...", fixed=TRUE)
regex("[:punc:]")
regex("[:punct:]")
gsub("[[:punct:]]", "", "Susan...")
id <- sapply(1:length(waves.text),
function(i) substr(waves.text[i],
name_locs[i, 1],
name_locs[i, 2])) %>%
gsub("\' said ", "", .) %>%
gsub("[[:punct:]]", "", .)
id
waves.passages <- grep("^[^\']", waves.text)
waves.passages
which(!is.na(id))
which(!is.na(id)) %in% waves.passages
which(which(!is.na(id)) %in% waves.passages)
waves.text[128]
waves.text[162]
which(!is.na(id))[162]
waves.text[258]
waves.passages <- grep("^\(*[^\']", waves.text)
waves.passages <- grep("^[^\(]*[^\']", waves.text)
waves.passages <- grep("^[^(]*[^\']", waves.text)
which(which(!is.na(id)) %in% waves.passages)
spoken.passages <- grep("^(*\'", waves.text)
spoken.passages <- grep("^[(]*\'", waves.text)
spoken.passages
z <- waves.text[-spoken.passages]
head(z)
z[1:10]
z[11:20]
z[21:30]
z[31:40]
z[41:50]
?xpathApply
?"htmlTreeParse"
z <- xpathApply(doc.html, '//i', xmlValue)
z <- xpathApply(doc.html, '/p/i', xmlValue)
z <- xpathApply(doc.html, '//p//i', xmlValue)
?xpathApply
z <- xpathApply(doc.html, '//p', xmlValue)
z <- getNodeSet(doc.html, '//p', xmlValue)
italic.text <- unlist(xpathApply(doc.html, '//p//i', xmlValue)) %>%
gsub('\\n', ' ', .) %>%
gsub('\r', '', .)
which(doc.text %in% italic.text)
doc.text[which(doc.text %in% italic.text)]
doc.text[95]
italic.text[6]
italic.text[7]
italic.text <- unlist(xpathApply(doc.html, '<p><i>')) %>%
gsub('\\n', ' ', .) %>%
gsub('\r', '', .)
italic.text[6] <- paste(italic.text[6], italic.text[7])
italic.text[7] <- NULL
italic.text[7] <- NA
italic.text
doc.text[which(doc.text %in% italic.text)]
length(doc.text[which(doc.text %in% italic.text)])
italic.tex[23]
italic.text[23]
italic.text[24]
italic.text[25]
italic.text[30]
italic.text[31]
italic.text[31:35]
italic.text[31] <- patse(italic.text[31:35])
italic.text[31] <- paste(italic.text[31:35])
italic.text[31]
italic.text[32]
paste(italic.text[31:35])
paste(italic.text[31:35], collapse="")
paste(italic.text[31:35], collapse=" ")
italic.text[31] <- paste(italic.text[31:35], collapse=" ")
italic.text[32:35] <- NA
length(doc.text[which(doc.text %in% italic.text)])
bad.lines <- which(doc.text == doc.text[1])
waves.text <- doc.text[-bad.lines]
head(waves.text)
waves.text <- waves.text[2:length(waves.text)]
tail(waves.text)
waves.text <- waves.text[1:(length(waves.text)-2)]
library(stringr)
name_locs <- str_locate(pattern = "\' said [A-Z][a-z]*[:punct:]", waves.text)
id <- sapply(1:length(waves.text),
function(i) substr(waves.text[i],
name_locs[i, 1],
name_locs[i, 2])) %>%
gsub("\' said ", "", .) %>%
gsub("[[:punct:]]", "", .)
waves.passages <- which(waves.text %in% italic.text)
id[waves.passages] <- "WAVES"
curr.speaker <- "WAVES"
for(i in 1:length(id)){
if(!is.na(id[i])) curr.speaker <- id[i]
else id[i] <- curr.speaker
}
id
waves.text <- gsub("\' said [A-Z][a-z]*[[:punc:]]", " ", waves.text)
waves.text <- gsub("\' said [A-Z][a-z]*[[:punct:]]", " ", waves.text)
head(waves.text)
waves.text <- gsub("\' said [A-Z][a-z]*[[:punct:]][\']*", " ", waves.text)
library(tokenizers)
all.words <- lapply(waves.text, tokenize_words) %>% unlist %>% unique %>% sort
waves.text.words <- lapply(waves.text, tokenize_words) %>%
lapply(., table) %>%
lapply(., function(x) {
x <- as.data.frame(x)
rownames(x) <- x$Var1
x$Var1 <- NULL
as.data.frame(t(x))
}) %>%
rbind.fill
waves.text.words$speaker <- id
waves.text.words$id <- paste0(id, 1:length(id))
View(waves.text.words)
?rbind.fill
library(wordcloud)
install.packages("wordcloud")
install.packages("slam")
install.packages("installr")
source('~/Coding Adventures/waves/1_preprocess_text.R')
library(wordcloud)
library(RColorBrewer)
library(tokenizers)
library(dplyr)
library(tm)
getCharacterCloud <- function(char.name, word.matrix, exclude=NULL,
scale=c(8, .5), min.freq=3,
max.words=100, colors="black"){
char.rows <- word.matrix %>%
filter(SPEAKER == char.name) %>%
select(-SPEAKER)
char.words <- colSums(x = char.rows, na.rm=TRUE) %>%
.[!(names(.) %in% exclude)]
wordcloud(names(char.words), char.words, scale, min.freq, max.words,
random.order=FALSE, colors=colors)
}
stopwds1 <- tokenizers::stopwords("en")
stopwds2 <- tm:stopwords("en")
stopwds3 <- tm:stopwords("SMART")
stopwds2 <- tm::stopwords("en")
stopwds3 <- tm::stopwords("SMART")
pal <- brewer.pal(6,"BuGn") %>%
.[-(1:3)]
getCharacterCloud("Rhoda", waves.text.words[-ncol(waves.text.words)],
c(stopwds2), scale=c(4, .5), max.words=100,
colors=pal)
getCharacterCloud("Rhoda", waves.text.words[-ncol(waves.text.words)],
c(stopwds2), scale=c(4, .5), max.words=100,
colors=pal)
getCharacterCloud("Rhoda", waves.text.words[-ncol(waves.text.words)],
stopwds1, 100, pal)
png("RhodaWordCloud_test.png")
getCharacterCloud("Rhoda", waves.text.words[-ncol(waves.text.words)],
c(stopwds2), scale=c(4, .5), max.words=100,
colors=pal)
dev.off()
getCharacterCloud("Rhoda", waves.text.words[-ncol(waves.text.words)],
c(stopwds3), scale=c(4, .5), max.words=100,
colors=pal)
png("RhodaWordCloud_test.png")
getCharacterCloud("Rhoda", waves.text.words[-ncol(waves.text.words)],
c(stopwds3), scale=c(4, .5), max.words=100,
colors=pal)
dev.off()
getCharacterCloud("Louis", waves.text.words[-ncol(waves.text.words)],
c(stopwds3), scale=c(4, .5), max.words=100,
colors=pal)
warnings()
getCharacterCloud("Louis", waves.text.words[-ncol(waves.text.words)],
c(stopwds3), scale=c(4, .5), max.words=100,
colors=pal)
getCharacterCloud("Jinny", waves.text.words[-ncol(waves.text.words)],
c(stopwds3), scale=c(4, .5), max.words=100,
colors=pal)
?brewer.pal
display.brewer.pal(6, "OrRd")
pal <- brewer.pal(6,"OrRd") %>%
.[-(1:2)]
pal2 <- brewer.pal(6,"OrRd") %>%
.[-(1:2)]
png("JinnyWordCloud_test.png")
getCharacterCloud("Jinny", waves.text.words[-ncol(waves.text.words)],
c(stopwds3), scale=c(4, .5), max.words=100,
colors=pal2)
dev.off()
setwd("~/Coding Adventures/waves")
pal <- brewer.pal(6,"BuGn") %>%
.[-(1:3)]
png("RhodaWordCloud_test.png")
getCharacterCloud("Rhoda", waves.text.words[-ncol(waves.text.words)],
c(stopwds3), scale=c(4, .5), max.words=100,
colors=pal)
dev.off()
pal2 <- brewer.pal(6,"OrRd") %>%
.[-(1:2)]
png("JinnyWordCloud_test.png")
getCharacterCloud("Jinny", waves.text.words[-ncol(waves.text.words)],
c(stopwds3), scale=c(4, .5), max.words=100,
colors=pal2)
dev.off()
getCharacterCloud("Susan", waves.text.words[-ncol(waves.text.words)],
c(stopwds3), scale=c(4, .5), max.words=100,
colors=pal)
getCharacterCloud("Susan", waves.text.words[-ncol(waves.text.words)],
c(stopwds3), scale=c(3, .5), max.words=100,
colors=pal)
getCharacterCloud("Louis", waves.text.words[-ncol(waves.text.words)],
c(stopwds3), scale=c(3, .5), max.words=100,
colors=pal)
getCharacterCloud("Neville", waves.text.words[-ncol(waves.text.words)],
c(stopwds3), scale=c(3, .5), max.words=100,
colors=pal)
